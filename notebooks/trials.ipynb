{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaa715b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\odz-2\\\\Desktop\\\\itü\\\\bitirme\\\\2nd part\\\\exercise-chatbot-project\\\\exercise-chatbot-project'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dbe7b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "193b2048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API keys\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c92b3eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY= os.getenv(\"GOOGLE_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "GRADER_API_KEY = os.getenv(\"GRADER_API_KEY\")\n",
    "GENERATOR_GOOGLE_API_KEY = os.getenv(\"GENERATOR_GOOGLE_API_KEY\")\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "os.environ[\"GRADER_API_KEY\"] = GRADER_API_KEY\n",
    "os.environ[\"GENERATOR_GOOGLE_API_KEY\"] = GENERATOR_GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb347771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\odz-2\\anaconda3\\envs\\excbot\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c44ff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "index_name = \"fitness-chatbot\"\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384, # Dimension for 'all-MiniLM-L6-v2'\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "708ad92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 166 documents (7 TXTs and 2 PDFs).\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loaders = {\n",
    "    \".txt\": TextLoader,\n",
    "    \".pdf\": PyPDFLoader\n",
    "}\n",
    "\n",
    "\n",
    "def create_loader(file_path):\n",
    "    ext = os.path.splitext(file_path)[1]\n",
    "    if ext in loaders:\n",
    "        return loaders[ext](file_path)\n",
    "    return None\n",
    "\n",
    "path = \"data/knowledge_base/\"\n",
    "docs = []\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    loader = create_loader(os.path.join(path, file))\n",
    "    if loader:\n",
    "        docs.extend(loader.load())\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents (7 TXTs and 2 PDFs).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "becabe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split(docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=700,\n",
    "        chunk_overlap=100\n",
    "        )\n",
    "    text_chunks = text_splitter.split_documents(docs)\n",
    "    return text_chunks\\\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec25f99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 549\n"
     ]
    }
   ],
   "source": [
    "texts_chunks = text_split(docs)\n",
    "print(f\"Number of chunks: {len(texts_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bbf1b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_embeddings():\n",
    "    model_name = \"all-MiniLM-L6-v2\"\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        \n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "embedding = download_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8393b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the text chunks into embeddings and storing them in Pinecone vector store\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=texts_chunks,\n",
    "    embedding=embedding,\n",
    "    index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "175dee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    embedding=embedding,\n",
    "    index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73829877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_search(query):\n",
    "    # k=3 means \"find the top 3 most relevant paragraphs\"\n",
    "    results = docsearch.similarity_search(query, k=3)\n",
    "    for i, res in enumerate(results):\n",
    "        print(f\"\\n--- Result {i+1} (Source: {res.metadata.get('source')}) ---\")\n",
    "        print(res.page_content[:300] + \"...\")\n",
    "\n",
    "test_search(\"What are the guidelines for exercising with hypertension?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdcbfc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\odz-2\\anaconda3\\envs\\excbot\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\odz-2\\anaconda3\\envs\\excbot\\Lib\\site-packages\\langchain_tavily\\tavily_research.py:97: UserWarning: Field name \"output_schema\" in \"TavilyResearch\" shadows an attribute in parent \"BaseTool\"\n",
      "  class TavilyResearch(BaseTool):  # type: ignore[override, override]\n",
      "c:\\Users\\odz-2\\anaconda3\\envs\\excbot\\Lib\\site-packages\\langchain_tavily\\tavily_research.py:97: UserWarning: Field name \"stream\" in \"TavilyResearch\" shadows an attribute in parent \"BaseTool\"\n",
      "  class TavilyResearch(BaseTool):  # type: ignore[override, override]\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from graphs.state import GraphState\n",
    "from dotenv import load_dotenv\n",
    "from graphs.nodes.query_analysis import query_analyzer_node\n",
    "from graphs.nodes.retriever import retriever_node\n",
    "from graphs.nodes.generator import generator_node\n",
    "from graphs.nodes.web_search import web_search_node\n",
    "from graphs.nodes.safety_grader import safety_grader_node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e6f3b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "load_dotenv()\n",
    "\n",
    "def route_question(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determines if we go to the local VectorDB or the Web.\n",
    "    \"\"\"\n",
    "    if state.get(\"datasource\") == \"web_search\":\n",
    "        print(\"---ROUTING TO WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    elif state.get(\"datasource\") == \"none\":\n",
    "        print(\"---ROUTING TO GENERAL CHAT---\")\n",
    "        return \"just_chat\"\n",
    "    else:\n",
    "        print(\"---ROUTING TO LOCAL RETRIEVER---\")\n",
    "        return \"retrieve_local\"\n",
    "    \n",
    "def decide_to_finish(state):\n",
    "    if state[\"is_safe\"] == \"yes\":\n",
    "        return \"safe\"\n",
    "    return \"unsafe\"\n",
    "\n",
    "def route_after_generation(state: GraphState):\n",
    "    \"\"\"\n",
    "    Decide whether to grade the output or just finish.\n",
    "    \"\"\"\n",
    "    # If the intent was a greeting or general chat, just stop.\n",
    "    # We access this from the user_profile we stored in query_analysis.\n",
    "    user_profile = state.get(\"user_profile\", {})\n",
    "    if user_profile.get(\"intent\") in [\"greeting\", \"general_chat\"]:\n",
    "        return \"finish\"\n",
    "    \n",
    "    # Otherwise, it's a workout—run it by the doctor (grader)\n",
    "    return \"grade\"\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "memory = InMemorySaver()\n",
    "\n",
    "# 2. Add the Nodes you've created\n",
    "workflow.add_node(\"analyze_query\", query_analyzer_node)\n",
    "workflow.add_node(\"retrieve_local\", retriever_node)\n",
    "workflow.add_node(\"web_search\", web_search_node)\n",
    "workflow.add_node(\"generate_workout\", generator_node)\n",
    "workflow.add_node(\"safety_grader\", safety_grader_node)\n",
    "\n",
    "# Add Edges (Hallways)\n",
    "workflow.add_edge(START, \"analyze_query\")\n",
    "\n",
    "# Add the Decision Path\n",
    "workflow.add_conditional_edges(\n",
    "    \"analyze_query\",\n",
    "    route_question,\n",
    "    {\n",
    "        \"retrieve_local\": \"retrieve_local\",\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"just_chat\": \"generate_workout\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Both paths lead to the Generator\n",
    "workflow.add_edge(\"retrieve_local\", \"generate_workout\")\n",
    "workflow.add_edge(\"web_search\", \"generate_workout\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_workout\",\n",
    "    route_after_generation,\n",
    "    {\n",
    "        \"finish\": END,\n",
    "        \"grade\": \"safety_grader\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"safety_grader\",\n",
    "    decide_to_finish,\n",
    "    {\n",
    "        \"safe\": END,\n",
    "        \"unsafe\": \"generate_workout\"  # This creates the LOOP\n",
    "    }\n",
    ")\n",
    "\n",
    "# 4. Compile the Graph\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the graph structure in a Mermaid chart\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies, so it might fail\n",
    "    print(\"Graph compiled successfully (but couldn't render image).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33bc591a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---NODE: ANALYZING USER QUERY---\n",
      "---ROUTING TO LOCAL RETRIEVER---\n",
      "--- Finished Node: analyze_query ---\n",
      "---NODE: RETRIEVING & FILTERING---\n",
      "--- Finished Node: retrieve_local ---\n",
      "---NODE: GENERATING WORKOUT PLAN---\n",
      "--- Finished Node: generate_workout ---\n",
      "---NODE: GRADING SAFETY---\n",
      "--- Finished Node: safety_grader ---\n"
     ]
    }
   ],
   "source": [
    "# Test Input\n",
    "inputs = {\"question\": \"I have lower back pain and want to train my glutes.\"}\n",
    "\n",
    "# Config is required for thread-based memory\n",
    "config = {\"configurable\": {\"thread_id\": \"user_session_1\"}}\n",
    "\n",
    "# Run the graph and stream the updates\n",
    "for output in app.stream(inputs, config):\n",
    "    for key, value in output.items():\n",
    "        print(f\"--- Finished Node: {key} ---\")\n",
    "        # Optional: print(value) to see the state update from that node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e2641b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---NODE: ANALYZING USER QUERY---\n",
      "---ROUTING TO WEB SEARCH---\n",
      "--- Node: analyze_query ---\n",
      "---NODE: WEB SEARCH---\n",
      "--- Node: web_search ---\n",
      "---NODE: GENERATING WORKOUT PLAN---\n",
      "--- Node: generate_workout ---\n",
      "---NODE: GRADING SAFETY---\n",
      "--- Node: safety_grader ---\n"
     ]
    }
   ],
   "source": [
    "#since we provided the memory now, by using the same thread_id, we expect it to remember the previous query and answer\n",
    "#so we pull a follow-up input\n",
    "\n",
    "follow_up_inputs = {\"question\": \"Can you replace one of those exercises with something I can do at home?\"}\n",
    "\n",
    "# Using the SAME thread_id pulls the previous state back out of memory\n",
    "config = {\"configurable\": {\"thread_id\": \"user_session_1\"}}\n",
    "\n",
    "for output in app.stream(follow_up_inputs, config):\n",
    "    print(f\"--- Node: {list(output.keys())[0]} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f9c46f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TURN 1: INITIAL REQUEST ---\n",
      "---NODE: ANALYZING USER QUERY---\n",
      "---ROUTING TO LOCAL RETRIEVER---\n",
      "Node: analyze_query\n",
      "---NODE: RETRIEVING & FILTERING---\n",
      "Node: retrieve_local\n",
      "---NODE: GENERATING WORKOUT PLAN---\n",
      "Node: generate_workout\n",
      "---NODE: GRADING SAFETY---\n",
      "Node: safety_grader\n",
      "\n",
      "--- TURN 2: FOLLOW-UP (CONTEXT TEST) ---\n",
      "---NODE: ANALYZING USER QUERY---\n",
      "---ROUTING TO LOCAL RETRIEVER---\n",
      "Node: analyze_query\n",
      "---NODE: RETRIEVING & FILTERING---\n",
      "Node: retrieve_local\n",
      "---NODE: GENERATING WORKOUT PLAN---\n",
      "Node: generate_workout\n",
      "---NODE: GRADING SAFETY---\n",
      "Node: safety_grader\n",
      "\n",
      "--- FINAL VERIFICATION ---\n",
      "Total Messages in History: 4\n",
      "Latest Generation Peek: Absolutely! We can definitely make that a bodyweight-only routine. Focusing on controlled movements ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Define your configuration (Thread ID is the most important part)\n",
    "config = {\"configurable\": {\"thread_id\": \"comprehensive_test_002\"}}\n",
    "\n",
    "# --- TURN 1: Initial Request ---\n",
    "print(\"--- TURN 1: INITIAL REQUEST ---\")\n",
    "input_1 = {\"messages\": [HumanMessage(content=\"I have lower back pain and want to train glutes.\")]}\n",
    "for output in app.stream(input_1, config):\n",
    "    print(f\"Node: {list(output.keys())[0]}\")\n",
    "\n",
    "# --- TURN 2: Follow-up (The Memory Test) ---\n",
    "print(\"\\n--- TURN 2: FOLLOW-UP (CONTEXT TEST) ---\")\n",
    "# Notice we DON'T mention back pain here\n",
    "input_2 = {\"messages\": [HumanMessage(content=\"Can you make that a bodyweight-only version?\")]}\n",
    "for output in app.stream(input_2, config):\n",
    "    print(f\"Node: {list(output.keys())[0]}\")\n",
    "\n",
    "# --- TURN 3: Verification ---\n",
    "print(\"\\n--- FINAL VERIFICATION ---\")\n",
    "final_state = app.get_state(config)\n",
    "print(f\"Total Messages in History: {len(final_state.values['messages'])}\")\n",
    "print(f\"Latest Generation Peek: {final_state.values['generation'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084acb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elite Medical Fitness Coach is Online. (Type 'exit' to stop)\n",
      "--------------------------------------------------\n",
      "---NODE: ANALYZING USER QUERY---\n",
      "DEBUG: Query Analyzer produced dict: {'name': 'None', 'intent': 'workout_request', 'conditions': [], 'goals': ['quadriceps', 'hamstrings', 'glutes', 'calves'], 'workout_type': 'strength', 'level': 'None', 'emergency': False}\n",
      "---ROUTING TO LOCAL RETRIEVER---\n",
      "---NODE: RETRIEVING & FILTERING---\n",
      "DEBUG: AI extracted goals: ['quadriceps', 'hamstrings', 'glutes', 'calves']\n",
      "DEBUG: Found 12 safe exercises in JSON.\n",
      "---NODE: GENERATING WORKOUT PLAN---\n",
      "---NODE: GRADING SAFETY---\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# This is your conversation \"Key\"\n",
    "config = {\"configurable\": {\"thread_id\": \"final_user_test_session2\"}}\n",
    "\n",
    "print(\"Elite Medical Fitness Coach is Online. (Type 'exit' to stop)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"stop\"]:\n",
    "        break\n",
    "\n",
    "    # Send the message to the graph\n",
    "    inputs = {\"messages\": [HumanMessage(content=user_input)]}\n",
    "    \n",
    "    # We use stream so we can see when it's thinking\n",
    "    final_response = \"\"\n",
    "    for output in app.stream(inputs, config):\n",
    "        for key, value in output.items():\n",
    "            if key == \"retrieve_local\":\n",
    "                print(f\"DEBUG: Found {len(value.get('safe_exercises', []))} safe exercises in JSON.\")\n",
    "            # We look for the generator node or the final state update\n",
    "            if \"generation\" in value:\n",
    "                final_response = value[\"generation\"]\n",
    "    \n",
    "    print(f\"\\nCoach: {final_response}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "excbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
